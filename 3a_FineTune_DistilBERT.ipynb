{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3957141c",
   "metadata": {},
   "source": [
    "# üß† Mini Workshop 3: Fine-Tuning DistilBERT for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a6af2",
   "metadata": {},
   "source": [
    "Welcome to your first language model fine-tuning task! In this notebook, you'll take a pretrained model that already knows English and teach it to recognize **sentiment** from short user reviews.\n",
    "\n",
    "We'll use **DistilBERT**, a smaller and faster version of BERT, and fine-tune it on the **Yelp Polarity** dataset using Hugging Face's `transformers` and `datasets` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ce7ff",
   "metadata": {},
   "source": [
    "## üìö Part 1: What Is Fine-Tuning?\n",
    "Pretrained language models like BERT have already been trained on a huge amount of text. They're great at understanding English.\n",
    "\n",
    "> In this workshop, we‚Äôll use a pretrained model and **fine-tune** it ‚Äî which means updating it slightly to learn a new skill: recognizing sentiment.\n",
    "\n",
    "This is the same idea behind many state-of-the-art models you hear about today ‚Äî adapting a general-purpose model to a specific task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd43141",
   "metadata": {},
   "source": [
    "## üß™ Part 2: Load and Tokenize Dataset\n",
    "Before we can train our model, we need to prepare the text data so it can be understood by a neural network.\n",
    "\n",
    "- We're using a small subset of the **Yelp Polarity** dataset, which contains short reviews labeled as positive or negative.\n",
    "- The **tokenizer** takes each review and breaks it down into numerical input IDs (tokens), adds padding if needed, and creates an **attention mask** that tells the model which parts of the input are real vs. padding.\n",
    "- We'll use the tokenizer that matches our pretrained model (DistilBERT).\n",
    "\n",
    "This step turns raw text into model-ready inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2066ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small portion of the Yelp Polarity dataset for quick experimentation\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load 1,000 training examples from the Yelp Polarity dataset\n",
    "    # and split it into training and testing sets\n",
    "dataset = load_dataset(\"yelp_polarity\", split=\"train[:1000]\").train_test_split(test_size=0.2)\n",
    "\n",
    "# Load the tokenizer that matches our model (DistilBERT)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length')\n",
    "\n",
    "# Tokenize the dataset\n",
    "    # and remove the original text column\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "tokenized_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d1ba1",
   "metadata": {},
   "source": [
    "## üß† Part 3: Load Pretrained Model\n",
    "Now we load a **DistilBERT model** that‚Äôs already been trained on a huge collection of English text.\n",
    "\n",
    "- This model already understands grammar, vocabulary, and context to a decent degree.\n",
    "- We'll fine-tune it for our specific task: **classifying reviews as positive or negative**.\n",
    "- We use a special version of DistilBERT that includes a **classification head** ‚Äî a final layer that outputs class predictions.\n",
    "\n",
    "Think of this as giving the model a final coaching session focused only on Yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e50e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76078f",
   "metadata": {},
   "source": [
    "## üîÅ Part 4: Fine-Tune the Model\n",
    "\n",
    "We're now ready to train our model using Hugging Face's `Trainer` class ‚Äî a powerful tool that takes care of most of the training loop for us.\n",
    "\n",
    "Here‚Äôs what each new component does:\n",
    "\n",
    "- **`TrainingArguments`**: This is where we configure how training should work. We set:\n",
    "  - `output_dir`: where to save the model\n",
    "  - `eval_strategy=\"epoch\"`: run evaluation at the end of every epoch\n",
    "  - `learning_rate`: how fast the model updates its weights\n",
    "  - `per_device_train_batch_size`: batch size during training\n",
    "  - `num_train_epochs`: how many passes through the dataset\n",
    "  - `logging_steps`: how often to print logs during training\n",
    "\n",
    "- **`DataCollatorWithPadding`**: This handles automatic padding of inputs so that batches can be formed correctly. It makes sure each sentence is padded to the length of the longest one in the batch.\n",
    "\n",
    "- **`Trainer`**: This is the high-level training loop. It handles:\n",
    "  - Feeding batches to the model\n",
    "  - Computing loss\n",
    "  - Running backpropagation\n",
    "  - Evaluating on the test set\n",
    "\n",
    "Once it‚Äôs set up, we simply call `.train()` and the whole training process runs for us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6ac27",
   "metadata": {},
   "source": [
    "## üîç Part 5: Make Predictions!\n",
    "\n",
    "Now that we have our fine-tuned model, we can see its performance on some sample statements.\n",
    "\n",
    "Try out your own statements below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977263a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Tokenize and move to model's device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Run model\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.softmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Format results\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "    predicted_class = labels[probs.index(max(probs))]\n",
    "    confidence = max(probs)\n",
    "\n",
    "    print(f\"\\nüìù Input: {text}\")\n",
    "    print(f\"ü§ñ Prediction: {predicted_class} ({confidence*100:.2f}% confidence)\")\n",
    "    print(f\"üìä Raw probabilities: {dict(zip(labels, [f'{p:.4f}' for p in probs]))}\")\n",
    "\n",
    "\n",
    "predict_sentiment(\"This place was amazing! The food was delicious and the service was great.\")\n",
    "predict_sentiment(\"I had a terrible experience. The food was cold and the staff was rude.\")\n",
    "predict_sentiment(\"The ambiance was nice, but the food was just ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own text here!\n",
    "predict_sentiment(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb33be",
   "metadata": {},
   "source": [
    "### üéì Recap\n",
    "Congratulations! You just fine-tuned a real language model on a real task.\n",
    "\n",
    "Now you can classify text using your customized DistilBERT model. In the next notebook, we‚Äôll try something more expressive ‚Äî emotion classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
