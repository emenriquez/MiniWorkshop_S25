{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2f1fbde",
      "metadata": {
        "id": "d2f1fbde"
      },
      "source": [
        "# Mini Workshop 2: Sentiment Classification\n",
        "Welcome to your second AI mini-workshop! In this notebook, you'll learn how to classify the sentiment of text â€” determining whether a sentence is **positive** or **negative**. You'll work with real datasets, use a powerful language model tokenizer, visualize sentence embeddings, and train a simple classifier.\n",
        "\n",
        "Don't worry â€” everything will be explained step-by-step, and you'll have space to try things out on your own throughout."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6852764",
      "metadata": {
        "id": "e6852764"
      },
      "source": [
        "## Part 1: What is Sentiment Analysis?\n",
        "Sentiment analysis is a type of text classification that identifies the emotional tone of text. It's used to analyze:\n",
        "- Product reviews\n",
        "- Movie ratings\n",
        "- Social media posts\n",
        "\n",
        "Today we'll use a real dataset (Yelp Polarity) and explore how a computer can learn to classify these sentiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d29ef7",
      "metadata": {
        "id": "49d29ef7"
      },
      "source": [
        "## Part 2: Load the Yelp Polarity Dataset\n",
        "We'll use the Yelp Polarity dataset, which contains short reviews labeled as positive or negative. We're using a subset for faster training and easier understanding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if not gpu_available:\n",
        "  print(\"WARNING: GPU runtime is not enabled. Please go to 'Runtime' > 'Change Runtime Type' > 'GPU' to enable it for faster training.\")\n",
        "else:\n",
        "  print(\"GPU runtime is enabled.\")\n",
        "  # Print the GPU device name if available\n",
        "  print(\"GPU Device Name:\", tf.config.list_physical_devices('GPU')[0].name)"
      ],
      "metadata": {
        "id": "XAjjFPeJlNjW"
      },
      "id": "XAjjFPeJlNjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets -q # will give some conflict errors, but should be ok (updated 5/2025)"
      ],
      "metadata": {
        "id": "QluzrEMrBesM"
      },
      "id": "QluzrEMrBesM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac07667",
      "metadata": {
        "id": "aac07667"
      },
      "outputs": [],
      "source": [
        "# This section loads a dataset of Yelp reviews labeled as positive or negative.\n",
        "from datasets import load_dataset\n",
        "# Load a small sample (first 200) from the Yelp Polarity training split for speed\n",
        "small_dataset = load_dataset(\"yelp_polarity\", split=\"train[:200]\")\n",
        "\n",
        "# Print the first example to see the structure of the data (dictionary with 'text' and 'label')\n",
        "print(small_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9fddcee",
      "metadata": {
        "id": "b9fddcee"
      },
      "source": [
        "### Let's Explore\n",
        "Print the first 5 examples and their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3660b2af",
      "metadata": {
        "id": "3660b2af"
      },
      "outputs": [],
      "source": [
        "# Loop over the first 5 examples in the small_dataset\n",
        "for i in range(5):\n",
        "    # Print the i-th example from the dataset (a dictionary with 'text' and 'label')\n",
        "    print(small_dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26383c17",
      "metadata": {
        "id": "26383c17"
      },
      "source": [
        "## Part 3: Tokenization with DistilBERT\n",
        "Before a model can work with text, the text must be converted into numbers. Weâ€™ll use a **tokenizer** to convert each sentence into tokens that represent subword units. These tokens are then mapped to unique IDs that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc69b6d",
      "metadata": {
        "id": "6fc69b6d"
      },
      "outputs": [],
      "source": [
        "# Before using text in a model, we must convert it to numbers (tokens).\n",
        "    # We use a pre-trained tokenizer from Hugging Face's transformers library.\n",
        "from transformers import AutoTokenizer\n",
        "# Create a tokenizer object for the 'distilbert-base-uncased' model\n",
        "    # This will lowercase and split text into subword tokens\n",
        "    # (The tokenizer knows how to convert text to numbers for the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Get the text of the first review in the dataset\n",
        "example = small_dataset[0]['text']\n",
        "\n",
        "# Tokenize the text: convert to input IDs (numbers), pad/truncate as needed\n",
        "tokens = tokenizer(example, padding=True, truncation=True)\n",
        "\n",
        "# Print the original text and the list of token IDs\n",
        "print(f\"Original: {example}\\nTokens: {tokens['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c503f5f1",
      "metadata": {
        "id": "c503f5f1"
      },
      "source": [
        "### âœï¸ Try it Yourself\n",
        "Use the tokenizer on your own short sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d771a91",
      "metadata": {
        "id": "5d771a91"
      },
      "outputs": [],
      "source": [
        "# Try tokenizing your own sentence\n",
        "your_sentence = \"\"\n",
        "\n",
        "# Tokenize your sentence (convert to input IDs, pad/truncate as needed)\n",
        "tokens = tokenizer(your_sentence, padding=True, truncation=True)\n",
        "\n",
        "# Print the list of token IDs for your sentence\n",
        "print(tokens['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe935a50",
      "metadata": {
        "id": "fe935a50"
      },
      "source": [
        "# Part 4: Three Approaches to Text Classification\n",
        "Now that you've learned how to tokenize and visualize text embeddings, it's time to train some actual classifiers!\n",
        "\n",
        "In this section, we'll explore **three different ways** to build a sentiment classifier from scratch. You'll see how different models represent text and how well they perform â€” and we'll visualize them using t-SNE just like before.\n",
        "\n",
        "Each method has a different strength, and we'll compare them side by side."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c4e9b6",
      "metadata": {
        "id": "27c4e9b6"
      },
      "source": [
        "## Model Overview\n",
        "| Model Type                        | Learns Word Order? | Custom Embeddings? | Fast to Train? |\n",
        "|----------------------------------|--------------------|---------------------|----------------|\n",
        "| 1. Bag-of-Words + Linear         | âŒ No              | âŒ No               | âœ… Yes         |\n",
        "| 2. Embeddings + Mean Pooling     | âŒ No              | âœ… Yes              | âœ… Yes         |\n",
        "| 3. LSTM or GRU                   | âœ… Yes             | âœ… Yes              | âš ï¸ Slower      |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57112a99",
      "metadata": {
        "id": "57112a99"
      },
      "source": [
        "### ðŸ“Œ Teaching Goals:\n",
        "- **Model 1**: Understand that you can classify text just by seeing which words are present.\n",
        "- **Model 2**: Learn how models can learn word meanings via embeddings.\n",
        "- **Model 3**: See how sequential models capture the order of words.\n",
        "\n",
        "We'll train all three, but **you'll only have exercises for Model 2** to stay on schedule."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85a25a4",
      "metadata": {
        "id": "c85a25a4"
      },
      "source": [
        "## ðŸ§ª Model 1: Bag-of-Words + Linear Classifier\n",
        "This is the simplest kind of text classifier. We convert each sentence into a binary vector that marks whether a word is present or not. Then we feed this vector into a linear layer that makes a prediction.\n",
        "\n",
        "#### Model Objective:\n",
        "`This is like checking which words are present. No word order, but fast and intuitive.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e78fed",
      "metadata": {
        "id": "76e78fed"
      },
      "outputs": [],
      "source": [
        "# needed imports for the next section\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Build a basic vocabulary from the training data\n",
        "# This function counts the most common words in the dataset and assigns each a unique index.\n",
        "def build_vocab(texts, max_vocab=1000):\n",
        "    from collections import Counter\n",
        "    word_counts = Counter()  # Create a counter for word frequencies\n",
        "    for text in texts:\n",
        "        word_counts.update(text.lower().split())  # Count each word in the text\n",
        "    most_common = word_counts.most_common(max_vocab)  # Get the most frequent words\n",
        "    vocab = {word: idx for idx, (word, _) in enumerate(most_common)}  # Map word to index\n",
        "    return vocab\n",
        "\n",
        "# This function converts a sentence into a Bag-of-Words vector (1 if word is present, 0 otherwise)\n",
        "def bow_vector(sentence, vocab):\n",
        "    import torch\n",
        "    vector = torch.zeros(len(vocab))  # Start with a vector of zeros\n",
        "    for word in sentence.lower().split():\n",
        "        if word in vocab:\n",
        "            vector[vocab[word]] = 1.0  # Set to 1 if the word is in the vocab\n",
        "    return vector\n",
        "\n",
        "# Create the vocabulary from the training data\n",
        "vocab = build_vocab(small_dataset['text'])\n",
        "\n",
        "\n",
        "# Prepare Bag-of-Words features for all training sentences\n",
        "X_bow = torch.stack([bow_vector(text, vocab) for text in small_dataset['text']])  # Shape: (num_examples, vocab_size)\n",
        "# Prepare labels as a tensor (shape: num_examples x 1)\n",
        "y_bow = torch.tensor(small_dataset['label'], dtype=torch.float32).unsqueeze(1) # Add a dimension for labels\n",
        "\n",
        "# Set up mini-batch DataLoader for Bag-of-Words features\n",
        "batch_size = 32  # Number of examples per batch\n",
        "bow_dataset = TensorDataset(X_bow, y_bow)  # Pair features and labels\n",
        "bow_loader = DataLoader(bow_dataset, batch_size=batch_size, shuffle=True)  # Shuffle for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3e01d7",
      "metadata": {
        "id": "9c3e01d7"
      },
      "outputs": [],
      "source": [
        "# Define a simple neural network: linear layer + sigmoid for binary classification\n",
        "model1 = nn.Sequential(\n",
        "    nn.Linear(len(vocab), 1),  # Input size = vocab size, output = 1 (probability)\n",
        "    nn.Sigmoid()               # Output between 0 and 1\n",
        ")\n",
        "\n",
        "# Set up loss function (binary cross-entropy) and optimizer (Adam)\n",
        "loss_fn = nn.BCELoss() # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01, weight_decay=1e-4) # Adam optimizer with weight decay\n",
        "\n",
        "# Train the model using mini-batch gradient descent\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in bow_loader:\n",
        "        y_pred = model1(X_batch)  # Forward pass: predict probabilities\n",
        "        loss = loss_fn(y_pred, y_batch)  # Compute loss\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Backpropagation\n",
        "        optimizer.step()       # Update weights\n",
        "        total_loss += loss.item() * X_batch.size(0)  # Accumulate loss\n",
        "    if epoch % 10 == 0:\n",
        "        avg_loss = total_loss / len(bow_loader.dataset)\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a977eb86",
      "metadata": {
        "id": "a977eb86"
      },
      "outputs": [],
      "source": [
        "# Visualize the Bag-of-Words vectors using t-SNE (2D projection)\n",
        "bow_embeds = TSNE(n_components=2, random_state=42).fit_transform(X_bow.numpy())  # Reduce to 2D\n",
        "colors = ['red' if label == 0 else 'blue' for label in small_dataset['label']]  # Color by label\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(bow_embeds[:, 0], bow_embeds[:, 1], c=colors, alpha=0.6)  # Plot points\n",
        "plt.title(\"t-SNE of BOW Sentence Representations\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c699f534",
      "metadata": {
        "id": "c699f534"
      },
      "source": [
        "## ðŸ§ª Model 2: Trainable Embeddings + Mean Pooling\n",
        "This model maps each word to a **learned embedding vector** (like a trainable word meaning). We take the average of all word vectors in a sentence and use that as our sentence representation.\n",
        "\n",
        "### ðŸ“Œ Teaching Goal:\n",
        "`Now the model learns how to represent words in vector space.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224909a0",
      "metadata": {
        "id": "224909a0"
      },
      "outputs": [],
      "source": [
        "# Each word in the vocab gets a unique integer ID for embedding models\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "# This function converts a sentence to a list of word IDs (only words in vocab)\n",
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab[word] for word in sentence.lower().split() if word in vocab]\n",
        "\n",
        "# Convert all training sentences to lists of word IDs\n",
        "encoded_data = [encode_sentence(text, token_to_id) for text in small_dataset['text']]\n",
        "# Find the length of the longest sentence in the dataset\n",
        "max_len = max(len(seq) for seq in encoded_data)\n",
        "# Pad all sequences to the same length (with zeros for missing words)\n",
        "padded_data = [seq + [0]*(max_len - len(seq)) for seq in encoded_data]\n",
        "\n",
        "# Convert to tensors for PyTorch\n",
        "X_tensor = torch.tensor(padded_data)  # Shape: (num_examples, max_len)\n",
        "y_tensor = torch.tensor(small_dataset['label'], dtype=torch.float32).unsqueeze(1)  # Shape: (num_examples, 1)\n",
        "\n",
        "# Set up mini-batch DataLoader for sequence models\n",
        "batch_size = 32\n",
        "seq_dataset = TensorDataset(X_tensor, y_tensor)  # Pair features and labels\n",
        "seq_loader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True)  # Shuffle for training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7c7de0",
      "metadata": {
        "id": "fc7c7de0"
      },
      "source": [
        "#### Mini-challenge! Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ebfd7b1",
      "metadata": {
        "id": "2ebfd7b1"
      },
      "outputs": [],
      "source": [
        "# Define a simple mean-pooling embedding model\n",
        "class MeanPoolModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)  # Embedding layer\n",
        "        self.fc = nn.Linear(emb_dim, 1)  # Output layer\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)         # Shape: (batch, seq_len, emb_dim)\n",
        "        pooled = embeds.mean(dim=1)        # Mean over sequence length\n",
        "        return torch.sigmoid(self.fc(pooled))  # Output probability\n",
        "\n",
        "model2 = MeanPoolModel(vocab_size=len(vocab), emb_dim=50)\n",
        "\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# mini-challenge! What type of loss function should we use for this model?\n",
        "loss_fn = #### YOUR CODE HERE ####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71512b7e",
      "metadata": {
        "id": "71512b7e"
      },
      "source": [
        "Once you have entered your loss function in the code cell above, try training your model below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ce116f",
      "metadata": {
        "id": "04ce116f"
      },
      "outputs": [],
      "source": [
        "# Train the mean-pooling model using mini-batch gradient descent\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in seq_loader:\n",
        "        y_pred = model2(X_batch)  # Forward pass: predict probabilities\n",
        "        loss = loss_fn(y_pred, y_batch)  # Compute loss\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Backpropagation\n",
        "        optimizer.step()       # Update weights\n",
        "        total_loss += loss.item() * X_batch.size(0)  # Accumulate loss\n",
        "    if epoch % 10 == 0:\n",
        "        avg_loss = total_loss / len(seq_loader.dataset)\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feeae7c4",
      "metadata": {
        "id": "feeae7c4"
      },
      "source": [
        "### âœï¸ Showcase: Predict on a New Sentence\n",
        "Below is a function that:\n",
        "1. Tokenizes your input sentence\n",
        "2. Converts it to tensor format\n",
        "3. Runs it through the model\n",
        "4. Outputs the sentiment prediction\n",
        "\n",
        "Try it with your own statement! (results may not be fantastic with our small dataset...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc96722",
      "metadata": {
        "id": "ecc96722"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(sentence):\n",
        "    # Convert the input sentence to word IDs\n",
        "    encoded = encode_sentence(sentence, token_to_id)\n",
        "    # Pad the sequence to match training length\n",
        "    padded = encoded + [0]*(max_len - len(encoded))\n",
        "    x = torch.tensor([padded])  # Shape: (1, max_len)\n",
        "    with torch.no_grad():       # Disable gradient calculation for inference\n",
        "        prob = model2(x).item()  # Get the model's output (probability)\n",
        "    label = \"Positive\" if prob >= 0.5 else \"Negative\"\n",
        "    confidence = prob if prob >= 0.5 else 1 - prob\n",
        "    print(f\"\\nðŸ“ Input: {sentence}\")\n",
        "    print(f\"ðŸ¤– Prediction: {label} ({confidence*100:.2f}% confidence)\")\n",
        "    print(f\"ðŸ“Š Raw probability (positive): {prob:.4f}\")\n",
        "\n",
        "\n",
        "# Enter your own sentence to test the model below!\n",
        "predict_sentiment(\"I love this workshop!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ec7ffc",
      "metadata": {
        "id": "17ec7ffc"
      },
      "source": [
        "### Visualize the mean-pooled embeddings using t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5232cf24",
      "metadata": {
        "id": "5232cf24"
      },
      "outputs": [],
      "source": [
        "# t-SNE on pooled embeddings (mean-pooled sentence representations)\n",
        "with torch.no_grad():\n",
        "    pooled_vectors = model2.embedding(X_tensor).mean(dim=1)  # Get mean embedding for each sentence\n",
        "# Reduce the high-dimensional embeddings to 2D for visualization\n",
        "reduced = TSNE(n_components=2, random_state=42).fit_transform(pooled_vectors.numpy())\n",
        "# Assign colors based on sentiment label (red=negative, blue=positive)\n",
        "colors = ['red' if label == 0 else 'blue' for label in small_dataset['label']]\n",
        "\n",
        "# Plot the t-SNE visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced[:, 0], reduced[:, 1], c=colors, alpha=0.6)  # Plot points\n",
        "plt.title(\"t-SNE of Mean Pooled Embeddings\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6116acc7",
      "metadata": {
        "id": "6116acc7"
      },
      "source": [
        "## ðŸ§ª Model 3: GRU-Based Sequential Classifier\n",
        "This model introduces a **GRU (Gated Recurrent Unit)** to handle word order. Instead of treating words as a bag or averaging their embeddings, the GRU processes them **in sequence**.\n",
        "\n",
        "### ðŸ“Œ Teaching Goal:\n",
        "`Now we can use the order of words to inform predictions.`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035420bf",
      "metadata": {
        "id": "035420bf"
      },
      "source": [
        "#### Mini-Challenge! Optimizer\n",
        "\n",
        "Complete the code below with an optimizer to help our model weights update in response to training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8cc191d",
      "metadata": {
        "id": "b8cc191d"
      },
      "outputs": [],
      "source": [
        "# Define a GRU-based model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)  # Word embeddings\n",
        "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer\n",
        "        self.fc = nn.Linear(hidden_dim, 1)      # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)              # (batch, seq_len, emb_dim)\n",
        "        _, h_n = self.gru(embeds)               # h_n: (1, batch, hidden_dim)\n",
        "        return torch.sigmoid(self.fc(h_n.squeeze(0)))  # Output probability\n",
        "\n",
        "model3 = GRUModel(vocab_size=len(vocab), emb_dim=50, hidden_dim=64)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "# Mini-challenge! What type of optimizer will you choose for this model?\n",
        "    # What hyperparameter values should we try?\n",
        "optimizer = ### YOUR CODE HERE ###\n",
        "\n",
        "# Use the same seq_loader as for mean pooling\n",
        "# Mini-batch training loop\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in seq_loader:\n",
        "        y_pred = model3(X_batch) # Forward pass: predict probabilities\n",
        "        loss = loss_fn(y_pred, y_batch.view(-1, 1).float()) # Compute loss\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "        loss.backward() # Backpropagation\n",
        "        optimizer.step() # Update weights\n",
        "        total_loss += loss.item() * X_batch.size(0) # Accumulate loss\n",
        "    if epoch % 10 == 0:\n",
        "        avg_loss = total_loss / len(seq_loader.dataset) # Average loss over the dataset\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38dd5953",
      "metadata": {
        "id": "38dd5953"
      },
      "outputs": [],
      "source": [
        "# t-SNE on final GRU hidden states\n",
        "with torch.no_grad():\n",
        "    embeds = model3.embedding(X_tensor)\n",
        "    _, h_n = model3.gru(embeds)\n",
        "    vectors = h_n.squeeze(0)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced = tsne.fit_transform(vectors.numpy())\n",
        "colors = ['red' if label == 0 else 'blue' for label in small_dataset['label']]\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced[:, 0], reduced[:, 1], c=colors, alpha=0.6)\n",
        "plt.title(\"t-SNE of GRU Hidden State Representations\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47809ad",
      "metadata": {
        "id": "f47809ad"
      },
      "source": [
        "## Part 4b: Comparing Model Performance\n",
        "Now that you've trained three different sentiment classifiers, let's compare how they perform on the Yelp Polarity dataset. We'll look at both their accuracy and F1-score, and discuss the strengths and weaknesses of each approach.\n",
        "\n",
        "### Model Comparison Table\n",
        "| Model Type                        | Learns Word Order? | Custom Embeddings? | Fast to Train? | Typical Strengths                |\n",
        "|-----------------------------------|--------------------|--------------------|---------------|----------------------------------|\n",
        "| 1. Bag-of-Words + Linear          | âŒ No              | âŒ No              | âœ… Yes        | Simple, interpretable, fast      |\n",
        "| 2. Embeddings + Mean Pooling      | âŒ No              | âœ… Yes             | âœ… Yes        | Learns word meaning, compact     |\n",
        "| 3. GRU-Based Sequential           | âœ… Yes             | âœ… Yes             | âš ï¸ Slower     | Captures word order, more robust |\n",
        "\n",
        "### Discussion\n",
        "- **Bag-of-Words**: Fastest to train and easy to interpret, but ignores word order and subtle meaning.\n",
        "- **Mean Pooling Embeddings**: Learns word representations, but still ignores order. Often a strong baseline.\n",
        "- **GRU**: Can capture the order of words, which is important for nuanced sentiment, but takes longer to train.\n",
        "\n",
        "Let's see how they compare on the test set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c05369",
      "metadata": {
        "id": "34c05369"
      },
      "outputs": [],
      "source": [
        "# This section loads a sample of the official test set and evaluates all three models.\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load 200 test samples from the Yelp Polarity test split\n",
        "hf_test_dataset = load_dataset(\"yelp_polarity\", split=\"test[:200]\")\n",
        "X_test = [ex['text'] for ex in hf_test_dataset]\n",
        "y_test = [ex['label'] for ex in hf_test_dataset]\n",
        "\n",
        "# --- Bag-of-Words ---\n",
        "# Convert test sentences to Bag-of-Words vectors\n",
        "X_test_bow = torch.stack([bow_vector(text, vocab) for text in X_test])\n",
        "with torch.no_grad():\n",
        "    y_pred_bow = (model1(X_test_bow) > 0.5).int().view(-1).numpy()\n",
        "acc_bow = accuracy_score(y_test, y_pred_bow)\n",
        "f1_bow = f1_score(y_test, y_pred_bow)\n",
        "\n",
        "# --- Mean Pooling ---\n",
        "# Convert test sentences to padded token IDs\n",
        "encoded_test = [encode_sentence(text, token_to_id) for text in X_test]\n",
        "padded_test = [seq + [0]*(max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in encoded_test]\n",
        "X_test_tensor = torch.tensor(padded_test)\n",
        "with torch.no_grad():\n",
        "    y_pred_mean = (model2(X_test_tensor) > 0.5).int().view(-1).numpy()\n",
        "acc_mean = accuracy_score(y_test, y_pred_mean)\n",
        "f1_mean = f1_score(y_test, y_pred_mean)\n",
        "\n",
        "# --- GRU ---\n",
        "# Use the same padded test tensor as for mean pooling\n",
        "with torch.no_grad():\n",
        "    y_pred_gru = (model3(X_test_tensor) > 0.5).int().view(-1).numpy()\n",
        "acc_gru = accuracy_score(y_test, y_pred_gru)\n",
        "f1_gru = f1_score(y_test, y_pred_gru)\n",
        "\n",
        "# Print results for all models\n",
        "print(f\"Bag-of-Words:     Accuracy={acc_bow:.3f}, F1={f1_bow:.3f}\")\n",
        "print(f\"Mean Pooling:     Accuracy={acc_mean:.3f}, F1={f1_mean:.3f}\")\n",
        "print(f\"GRU Sequential:   Accuracy={acc_gru:.3f}, F1={f1_gru:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6c9313",
      "metadata": {
        "id": "df6c9313"
      },
      "source": [
        "### Summary of Results\n",
        "- **Bag-of-Words**: Quick and interpretable, but may miss subtle sentiment cues due to lack of word order and context.\n",
        "- **Mean Pooling Embeddings**: Captures more nuance by learning word meanings, but still ignores order.\n",
        "- **GRU**: Best at handling complex sentences and word order, but requires more computation.\n",
        "\n",
        "In practice, the best model depends on your needs: for speed and simplicity, Bag-of-Words or Mean Pooling may suffice; for more nuanced understanding, sequential models like GRU are preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66122f8b",
      "metadata": {
        "id": "66122f8b"
      },
      "source": [
        "## Part 5: Try Another Dataset (Optional Challenge)\n",
        "Hugging Face provides many sentiment datasets you can explore. Here are a few suggestions:\n",
        "\n",
        "| Dataset      | Description                        | Load Command                           |\n",
        "|--------------|------------------------------------|----------------------------------------|\n",
        "| IMDb         | Full movie reviews (binary)        | `load_dataset(\"imdb\")`                |\n",
        "| Rotten Tomatoes dataset (from the GLUE benchmark)| Positive/Negative Rotten Tomatoes reviews     | `load_dataset(\"rotten_tomatoes\")`       |\n",
        "| TweetEval    | Tweets (3-class sentiment)         | `load_dataset(\"tweet_eval\", \"sentiment\")` |\n",
        "\n",
        "Try loading one and tokenize the first few entries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8946c075",
      "metadata": {
        "id": "8946c075"
      },
      "outputs": [],
      "source": [
        "# This cell shows how to load a different sentiment dataset from Hugging Face\n",
        "alt_dataset = load_dataset(\"imdb\")\n",
        "print(alt_dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect the runtime\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "qzsWf-_qAj3K"
      },
      "id": "qzsWf-_qAj3K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}