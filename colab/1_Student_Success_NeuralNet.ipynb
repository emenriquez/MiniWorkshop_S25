{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35e92970",
      "metadata": {
        "id": "35e92970"
      },
      "source": [
        "# Mini Workshop 1: Predicting Student Success\n",
        "\n",
        "Welcome to your first neural network! In this notebook, you'll train a simple AI model to predict whether a student passes or fails based on how much they studied and slept.\n",
        "\n",
        "Later, you'll explore how to expand this into predicting actual letter grades (A, B, C, D, F)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80da297",
      "metadata": {
        "id": "a80da297"
      },
      "source": [
        "## Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a10033",
      "metadata": {
        "id": "a1a10033"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we need for this notebook\n",
        "import torch  # PyTorch: a popular deep learning library\n",
        "import torch.nn as nn  # nn: tools for building neural networks\n",
        "import matplotlib.pyplot as plt  # For plotting graphs\n",
        "import numpy as np  # For working with numbers and arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5153554f",
      "metadata": {
        "id": "5153554f"
      },
      "source": [
        "## Step 2: Define the Dataset (Pass/Fail)\n",
        "\n",
        "Here we will make our own toy dataset that simulates data we have collected on students that have passed or failed, depending on the number of hours they studied, and the number of hours they slept the night before an exam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694954da",
      "metadata": {
        "id": "694954da"
      },
      "outputs": [],
      "source": [
        "# Our dataset: each row is [hours_studied, hours_slept]\n",
        "X = torch.tensor([\n",
        "    [1.0, 2.0], [3.0, 1.0], [5.0, 5.0], [6.0, 4.0],\n",
        "    [8.0, 7.0], [2.0, 6.0], [7.0, 3.0]\n",
        "])\n",
        "# Labels: 0 = fail, 1 = pass\n",
        "y = torch.tensor([\n",
        "    [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc3f5b9",
      "metadata": {
        "id": "1bc3f5b9"
      },
      "source": [
        "## Step 3: Define the Model\n",
        "\n",
        "This is a very simple model that will help us to make predictions. It only consists of two inputs that are being linearly combined and fed into a Sigmoid function to produce an output between 0 (0%) and 1 (100%) probability that a student will pass based on the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052f6d01",
      "metadata": {
        "id": "052f6d01"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 1),  # Linear layer: 2 inputs (study, sleep) -> 1 output\n",
        "    nn.Sigmoid()      # Squash output to be between 0 and 1 (probability)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d1b7ac",
      "metadata": {
        "id": "e8d1b7ac"
      },
      "source": [
        "## Step 4: Loss and Optimization\n",
        "\n",
        "Now that we’ve defined the model, we need to tell it how to **learn** from its mistakes. This is where the **loss function** and **optimizer** come in:\n",
        "\n",
        "- **Loss Function (`BCELoss`)**:  \n",
        "  The loss function measures how far off the model's prediction is from the true label.  \n",
        "  - We're using **Binary Cross Entropy Loss**, which is commonly used for binary classification problems (like pass/fail or yes/no).\n",
        "  - The closer the predicted value is to the true label, the lower the loss.\n",
        "\n",
        "- **Optimizer (`Adam`)**:  \n",
        "  The optimizer updates the model’s weights based on the computed loss.\n",
        "  - We’re using the **Adam optimizer**, which is a popular and effective optimization algorithm.\n",
        "  - The `lr` (learning rate) controls how big each update step is — too small and training is slow; too large and it may overshoot.\n",
        "\n",
        "Together, the loss function tells the model how wrong it is, and the optimizer figures out how to adjust the model to do better next time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9dcbd0",
      "metadata": {
        "id": "2c9dcbd0"
      },
      "outputs": [],
      "source": [
        "# Set up how the model learns\n",
        "loss_fn = nn.BCELoss()  # Binary Cross Entropy: good for yes/no problems\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Adam: a way to update the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83248b1b",
      "metadata": {
        "id": "83248b1b"
      },
      "source": [
        "## Step 5: Visualizing the Decision Boundary\n",
        "\n",
        "After training a neural network, it's helpful to **see what it has learned**. One way to do this is by plotting a **decision boundary** — a line (or surface) that separates different prediction regions in the input space.\n",
        "\n",
        "This function creates a visualization of that boundary:\n",
        "\n",
        "- It builds a grid of (x, y) points covering the input space (in our case: hours studied vs. hours slept).\n",
        "- It asks the model to make predictions at each point on the grid.\n",
        "- It then colors the regions based on the model’s output — showing where the model predicts **pass (1)** or **fail (0)**.\n",
        "- Finally, it overlays the original data points so we can see how well the boundary matches reality.\n",
        "\n",
        "This is a powerful way to build intuition about what the neural network is doing under the hood!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce1b699",
      "metadata": {
        "id": "5ce1b699"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(model, X, y):\n",
        "    # This function draws the decision boundary of the model\n",
        "    x_min, x_max = 0, 10\n",
        "    y_min, y_max = 0, 10\n",
        "    # Create a grid of points to cover the input space\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        preds = model(grid).reshape(xx.shape)  # Model predictions for each point\n",
        "    plt.contourf(xx, yy, preds, levels=50, cmap='RdBu', alpha=0.6)  # Color regions\n",
        "    plt.scatter(X[:,0], X[:,1], c=y.squeeze(), cmap='RdBu', edgecolor='k')  # Plot data\n",
        "    plt.xlabel('Hours Studied')\n",
        "    plt.ylabel('Hours Slept')\n",
        "    plt.title('Decision Boundary')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6513576c",
      "metadata": {
        "id": "6513576c"
      },
      "source": [
        "## Step 6: Train the Model and Watch Decision Boundary Evolve\n",
        "\n",
        "Now it's time to **train the model and watch it learn**.\n",
        "\n",
        "This loop runs the full training process step-by-step, and also shows us how the decision boundary evolves:\n",
        "\n",
        "- Every few steps, we **plot the decision boundary** to see how the model is changing.\n",
        "- We also **track the loss**, which tells us how far off the model's predictions are from the correct labels.\n",
        "- On each step:\n",
        "  - The model makes predictions (`y_pred`)\n",
        "  - We calculate how wrong those predictions are (`loss`)\n",
        "  - We compute gradients and update the model weights to improve\n",
        "\n",
        "As training progresses:\n",
        "- The boundary becomes more accurate and aligns better with the data.\n",
        "- The loss values should go down — a sign that the model is improving.\n",
        "\n",
        "This is a great way to build **intuition** about how training works — not just as numbers, but visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595d2085",
      "metadata": {
        "id": "595d2085"
      },
      "outputs": [],
      "source": [
        "# Train the model and watch how it learns\n",
        "losses = []  # Keep track of loss at each step\n",
        "for epoch in range(101):  # Do 101 training steps\n",
        "    if epoch < 10 and epoch % 2 == 0:\n",
        "        plot_decision_boundary(model, X, y)  # Show boundary early in training\n",
        "    y_pred = model(X)  # Model's predictions for our data\n",
        "    loss = loss_fn(y_pred, y)  # How wrong is the model?\n",
        "    losses.append(loss.item())  # Save the loss\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()  # Compute new gradients\n",
        "    optimizer.step()  # Update the model\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')  # Print progress\n",
        "plot_decision_boundary(model, X, y)  # Final boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9d1408",
      "metadata": {
        "id": "9f9d1408"
      },
      "source": [
        "#### Plot the loss\n",
        "\n",
        "We can also use loss to visualize our model's performance during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1d0ac2",
      "metadata": {
        "id": "9d1d0ac2"
      },
      "outputs": [],
      "source": [
        "# Plot how the loss changed during training\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eaf3867",
      "metadata": {
        "id": "2eaf3867"
      },
      "source": [
        "## Step 7: Try It Yourself\n",
        "\n",
        "Create your own test example! Use values for `hours_studied` and `hours_slept` to see if the model thinks the student will pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b20066",
      "metadata": {
        "id": "f2b20066"
      },
      "outputs": [],
      "source": [
        "# Try your own input!\n",
        "your_input = torch.tensor([[3.0, 3.0]])  # Change these numbers to test different students\n",
        "print(\"Predicted pass probability:\", model(your_input).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672d3f9c",
      "metadata": {
        "id": "672d3f9c"
      },
      "source": [
        "## Bonus: Predicting Letter Grades (A–F)\n",
        "\n",
        "Let's take it a step further. What if we want to predict the actual letter grade a student will get? This turns our binary classification problem into a **multiclass classification** problem.\n",
        "\n",
        "### Mini Challenge!\n",
        "\n",
        "Can you fill in the input and output dimensions for `model_multi` below? Consider the data that will go in, and the number of classes we are trying to predict in the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351eff4e",
      "metadata": {
        "id": "351eff4e"
      },
      "outputs": [],
      "source": [
        "# Multiclass dataset: [hours_studied, hours_slept]\n",
        "X_multi = torch.tensor([\n",
        "    [1.0, 2.0], [3.0, 1.0], [5.0, 5.0], [6.0, 4.0], [8.0, 7.0], [7.0, 3.0],\n",
        "    [2.0, 3.0], [4.0, 2.0], [6.0, 2.0], [8.0, 2.0], [3.0, 7.0], [5.0, 8.0], [7.0, 8.0],\n",
        "    [9.0, 5.0], [1.0, 8.0], [4.0, 6.0], [6.0, 8.0], [8.0, 4.0]\n",
        "])\n",
        "# Labels: 0=F, 1=D, 2=C, 3=B, 4=A\n",
        "y_multi = torch.tensor([\n",
        "    0, 1, 2, 3, 4, 3,\n",
        "    0, 1, 2, 3, 1, 2, 4,\n",
        "    4, 0, 2, 4, 3\n",
        "])\n",
        "\n",
        "# Build a model for multiclass (grades)\n",
        "model_multi = nn.Sequential(\n",
        "    ### YOUR CODE HERE ###\n",
        "    # NOTE: no Sigmoid() layer will be used here, because softmax will be applied by the loss function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building a neural network for **multiclass classification** (predicting more than two possible classes, like letter grades A–F), we use a different loss function than for binary (yes/no) problems.\n",
        "\n",
        "- **BCELoss (Binary Cross Entropy Loss)** is designed for binary classification, where each example belongs to one of two classes (e.g., pass/fail). It expects the model to output a single probability (between 0 and 1) for each example, and uses a Sigmoid activation to squash the output.\n",
        "\n",
        "- **CrossEntropyLoss** is designed for multiclass classification, where each example belongs to one of several classes (e.g., grades 0–4). It expects the model to output a vector of raw scores (called logits), one for each class. CrossEntropyLoss internally applies the Softmax function to these logits to turn them into probabilities, and then compares them to the true class label."
      ],
      "metadata": {
        "id": "iESrHsU8qrK3"
      },
      "id": "iESrHsU8qrK3"
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "loss_fn_multi = nn.CrossEntropyLoss()  # Good for multiclass problems\n",
        "optimizer_multi = torch.optim.Adam(model_multi.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "3kvklkZypNSm"
      },
      "id": "3kvklkZypNSm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841c1828",
      "metadata": {
        "id": "841c1828"
      },
      "outputs": [],
      "source": [
        "# Train the multiclass model\n",
        "for epoch in range(100):\n",
        "    y_pred_multi = model_multi(X_multi)  # Model's predictions\n",
        "    loss = loss_fn_multi(y_pred_multi, y_multi)  # How wrong is the model?\n",
        "    optimizer_multi.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()  # Compute new gradients\n",
        "    optimizer_multi.step()  # Update the model\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")  # Print progress"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c70e9c1",
      "metadata": {
        "id": "2c70e9c1"
      },
      "source": [
        "### Try Your Own Grade Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6b4b43",
      "metadata": {
        "id": "ee6b4b43"
      },
      "outputs": [],
      "source": [
        "# Try your own grade example\n",
        "my_student = torch.tensor([[5.0, 2.0]])  # Change these numbers to test different students\n",
        "predicted_grade = torch.argmax(model_multi(my_student), dim=1)  # Pick the grade with highest predicted probability\n",
        "print(\"Predicted grade (0=F to 4=A):\", predicted_grade.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dea8b1d",
      "metadata": {
        "id": "1dea8b1d"
      },
      "source": [
        "### Visualizing the Multiclass Decision Boundary\n",
        "Let's plot the decision boundary for the multiclass model to see how it separates the different letter grades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913ec1ec",
      "metadata": {
        "id": "913ec1ec"
      },
      "outputs": [],
      "source": [
        "def plot_multiclass_decision_boundary(model, X, y, num_classes=5):\n",
        "    # Plot the decision boundary for multiclass model\n",
        "    x_min, x_max = 0, 10\n",
        "    y_min, y_max = 0, 10\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        preds = model(grid)\n",
        "        preds = torch.argmax(preds, dim=1).reshape(xx.shape)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.contourf(xx, yy, preds, levels=np.arange(num_classes+1)-0.5, cmap='Spectral', alpha=0.6)\n",
        "    scatter = plt.scatter(X[:,0], X[:,1], c=y, cmap='Spectral', edgecolor='k', s=80)\n",
        "    plt.xlabel('Hours Studied')\n",
        "    plt.ylabel('Hours Slept')\n",
        "    plt.title('Multiclass Decision Boundary (Grades)')\n",
        "    plt.colorbar(scatter, ticks=range(num_classes), label='Grade (0=F, 4=A)')\n",
        "    plt.show()\n",
        "\n",
        "def plot_multiclass_decision_boundary_continuous(model, X, y, num_classes=5):\n",
        "    # Plot a smooth, continuous decision boundary for multiclass model\n",
        "    x_min, x_max = 0, 10\n",
        "    y_min, y_max = 0, 10\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(grid)\n",
        "        probs = torch.softmax(logits, dim=1)  # Convert to probabilities\n",
        "        color_vals = (probs * torch.arange(num_classes, dtype=torch.float32)).sum(dim=1).reshape(xx.shape)  # Weighted sum for smooth color\n",
        "        preds = torch.argmax(probs, dim=1).reshape(xx.shape)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    cont = plt.contourf(xx, yy, color_vals, levels=np.linspace(-0.5, 4.5, 101), cmap='Spectral', alpha=0.9)\n",
        "    scatter = plt.scatter(X[:,0], X[:,1], c=y, cmap='Spectral', edgecolor='k', s=80)\n",
        "    plt.xlabel('Hours Studied')\n",
        "    plt.ylabel('Hours Slept')\n",
        "    plt.title('Multiclass Decision Boundary (Continuous Colors)')\n",
        "    cbar = plt.colorbar(cont, ticks=[0, 1, 2, 3, 4], boundaries=np.linspace(-0.5, 4.5, 101))\n",
        "    cbar.ax.set_yticklabels(['F', 'D', 'C', 'B', 'A'])  # Show letter grades\n",
        "    cbar.set_label('Predicted Grade')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e3c33d",
      "metadata": {
        "id": "b7e3c33d"
      },
      "outputs": [],
      "source": [
        "# Plot both types of decision boundaries\n",
        "plot_multiclass_decision_boundary(model_multi, X_multi, y_multi)\n",
        "plot_multiclass_decision_boundary_continuous(model_multi, X_multi, y_multi)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: disconnect the runtime\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "vkh6xiIlATxQ"
      },
      "id": "vkh6xiIlATxQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}