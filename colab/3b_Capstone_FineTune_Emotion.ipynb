{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4bca4352",
      "metadata": {
        "id": "4bca4352"
      },
      "source": [
        "# üéì Capstone: Fine-Tuning a Language Model for Emotion Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d951baa1",
      "metadata": {
        "id": "d951baa1"
      },
      "source": [
        "Welcome to the final workshop notebook! In this exercise, you'll fine-tune a pretrained transformer model on the **Emotion** dataset. This dataset includes short text samples labeled with one of six emotions:\n",
        "\n",
        "- joy, sadness, anger, fear, surprise, love\n",
        "\n",
        "You'll use all the skills you've practiced: tokenization, model loading, training, and evaluation ‚Äî plus new ideas like zero-shot classification and planning your own projects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6e5cac",
      "metadata": {
        "id": "0c6e5cac"
      },
      "source": [
        "## üìÇ Part 1: Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if not gpu_available:\n",
        "  print(\"WARNING: GPU runtime is not enabled. Please go to 'Runtime' > 'Change Runtime Type' > 'GPU' to enable it for faster training.\")\n",
        "else:\n",
        "  print(\"GPU runtime is enabled.\")\n",
        "  # Print the GPU device name if available\n",
        "  print(\"GPU Device Name:\", tf.config.list_physical_devices('GPU')[0].name)"
      ],
      "metadata": {
        "id": "tmeMlEMTtvxe"
      },
      "id": "tmeMlEMTtvxe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets -q"
      ],
      "metadata": {
        "id": "zSldIliYuXj0"
      },
      "id": "zSldIliYuXj0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86070dca",
      "metadata": {
        "id": "86070dca"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('emotion')\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e23f585",
      "metadata": {
        "id": "9e23f585"
      },
      "source": [
        "### üîç Challenge: How many examples are there per emotion?\n",
        "Try using `dataset['train'].features` and simple loops or `pandas` to summarize the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d8ee44",
      "metadata": {
        "id": "e1d8ee44"
      },
      "source": [
        "## ‚úÇÔ∏è Part 2: Tokenize the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef873f71",
      "metadata": {
        "id": "ef873f71"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['text'], truncation=True, padding='max_length')\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "tokenized = tokenized.remove_columns(['text'])\n",
        "tokenized.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead322dc",
      "metadata": {
        "id": "ead322dc"
      },
      "source": [
        "## üß† Part 3: Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917f3fc7",
      "metadata": {
        "id": "917f3fc7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# TODO: Load model with 6 output labels (for the 6 emotions in the dataset)\n",
        "model = ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa977d3",
      "metadata": {
        "id": "ffa977d3"
      },
      "source": [
        "## üöÄ Part 4: Fine-Tune the Model\n",
        "\n",
        "Fill in the arguments below to build your Trainer to fine-tune the model. Some parts are filled in for you already.\n",
        "\n",
        "**NOTE:** This is a larger dataset! So make sure not to try and train for too many epochs. Start with a small number (1 or 2) and go up from there if you need to.\n",
        "\n",
        "**Fill in all of the missing parameters, replacing the \"...\" placeholders below!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550459ba",
      "metadata": {
        "id": "550459ba"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy='steps', # Evaluate every eval_steps\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    learning_rate=...,\n",
        "    per_device_train_batch_size=...,\n",
        "    num_train_epochs=...,\n",
        "    weight_decay=...,\n",
        "    report_to='none',\n",
        "    logging_dir='./logs'\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# TODO: Fill in the Trainer constructor\n",
        "trainer = Trainer(\n",
        "    model=...,\n",
        "    args=...,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['validation'],\n",
        "    tokenizer=...,\n",
        "    data_collator=...\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd4bda0",
      "metadata": {
        "id": "dcd4bda0"
      },
      "source": [
        "## üìà Part 5: Evaluate and Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87e87ac0",
      "metadata": {
        "id": "87e87ac0"
      },
      "outputs": [],
      "source": [
        "def predict_emotion(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    probs = outputs.logits.softmax(dim=-1).squeeze().tolist()\n",
        "    labels = dataset['train'].features['label'].names\n",
        "    predicted = labels[probs.index(max(probs))]\n",
        "    print(f\"\\nüìù Input: {text}\")\n",
        "    print(f\"ü§ñ Predicted Emotion: {predicted} ({max(probs)*100:.2f}% confidence)\")\n",
        "    print(f\"üìä Probabilities: {dict(zip(labels, [f'{p:.3f}' for p in probs]))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "537ede6f",
      "metadata": {
        "id": "537ede6f"
      },
      "outputs": [],
      "source": [
        "# write in your own statement to evaluate the model!\n",
        "predict_emotion(\"I can't believe you did that!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660198ad",
      "metadata": {
        "id": "660198ad"
      },
      "source": [
        "## üîÆ Part 6: Zero-Shot Classification Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7846ceb5",
      "metadata": {
        "id": "7846ceb5"
      },
      "source": [
        "Zero-shot classification uses a model trained on **Natural Language Inference** to label text without any additional training. We'll use `facebook/bart-large-mnli`, which can reason about label descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a3b01c",
      "metadata": {
        "id": "e1a3b01c"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "zero_shot = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
        "\n",
        "candidate_labels = dataset['train'].features['label'].names\n",
        "text = \"I can‚Äôt believe you did that!\"\n",
        "result = zero_shot(text, candidate_labels=candidate_labels)\n",
        "print(f\"\\nüìù Input: {text}\")\n",
        "print(f\"ü§ñ Predicted: {result['labels'][0]} ({result['scores'][0]*100:.2f}% confidence)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358add59",
      "metadata": {
        "id": "358add59"
      },
      "source": [
        "### üí¨ Discussion: Compare the results from your fine-tuned model and the zero-shot model. Do they agree? Does one make more sense to you than the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978d1b65",
      "metadata": {
        "id": "978d1b65"
      },
      "source": [
        "## Part 7: What‚Äôs Next?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c2bf2e",
      "metadata": {
        "id": "54c2bf2e"
      },
      "source": [
        "You‚Äôve fine-tuned a real language model and explored zero-shot reasoning. What‚Äôs next?\n",
        "Here are some cool project ideas you can try:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2dc054",
      "metadata": {
        "id": "2c2dc054"
      },
      "source": [
        "| Task | Dataset | Link | Description |\n",
        "|------|---------|------|-------------|\n",
        "| Emotion Detection | `emotion` | [üîó](https://huggingface.co/datasets/emotion) | Classify tweets by emotional tone |\n",
        "| Sarcasm Detection | `sarcasm` | [üîó](https://huggingface.co/datasets/sarcasm) | Detect whether something is sarcastic |\n",
        "| Toxic Comments | `jigsaw_toxicity_pred` | [üîó](https://huggingface.co/datasets/jigsaw_toxicity_pred) | Label toxic or abusive language |\n",
        "| Tweet Tasks | `tweet_eval` | [üîó](https://huggingface.co/datasets/tweet_eval) | Sentiment, hate, stance, emojis |\n",
        "| News Topics | `ag_news` | [üîó](https://huggingface.co/datasets/ag_news) | Classify short news into topics |\n",
        "| Fake News | `liar` | [üîó](https://huggingface.co/datasets/liar) | Label political statements as true/false |\n",
        "| Dialogue Emotion | `daily_dialog` | [üîó](https://huggingface.co/datasets/daily_dialog) | Intent and emotion in conversations |\n",
        "| Hate Speech | `hate_speech18` | [üîó](https://huggingface.co/datasets/hate_speech18) | Detect hate in text posts |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df204b84",
      "metadata": {
        "id": "df204b84"
      },
      "source": [
        "## üåç How to Keep Exploring\n",
        "\n",
        "Now that you've fine-tuned and evaluated your first model, here‚Äôs how you can take your skills even further:\n",
        "\n",
        "- üîé **Explore new datasets**: [Hugging Face Datasets Hub](https://huggingface.co/datasets)\n",
        "- üß† **Try different models**: [Hugging Face Models Hub](https://huggingface.co/models)\n",
        "- üìö **Read the docs**: [Transformers Documentation](https://huggingface.co/docs/transformers)\n",
        "- üíª **Use GPUs for free**: Google Colab provides a free GPU runtime ‚Äî perfect for experimenting.\n",
        "- üöÄ **Share your work**: Upload your model to [Hugging Face Hub](https://huggingface.co/docs/hub) or build a simple demo with [Gradio](https://www.gradio.app/) or [Hugging Face Spaces](https://huggingface.co/spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44277038",
      "metadata": {
        "id": "44277038"
      },
      "source": [
        "## üí° Project Ideas by Theme\n",
        "\n",
        "If you're looking for inspiration, here are a few directions you can take your next model:\n",
        "\n",
        "| Theme | Ideas |\n",
        "|-------|-------|\n",
        "| üí¨ Emotion / Mental Health | Detect mood in journaling apps, supportive chatbot for students |\n",
        "| ‚ö†Ô∏è Content Moderation | Toxic comment filter, hate speech detector, safe content assistant |\n",
        "| ‚ú® Creativity | Poetry classifier, sarcasm generator, meme captioner |\n",
        "| üéÆ Games & Fun | NPC emotion predictor, story tone classifier, in-game chat sentiment |\n",
        "| üì¢ Advocacy | Fake news detector, flag biased language, classify protest messaging |\n",
        "| üó£Ô∏è Cultural NLP | Analyze slang, code-switching detection, dialect identification |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect the runtime\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "aDKLKSBGtrxR"
      },
      "id": "aDKLKSBGtrxR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}